# HW07: Отчёт о кластеризации

## 1. Цель работы

Исследовать различные методы кластеризации (KMeans, DBSCAN, Agglomerative Clustering) на синтетических датасетах, оценить их качество с помощью внутренних метрик и провести анализ устойчивости результатов.

---

## 2. Описание данных

### Dataset 1: S07-hw-dataset-01.csv
- **Размер**: ~1000 образцов, ~10-15 числовых признаков
- **Характеристика**: числовые признаки в разных шкалах + шумовые признаки
- **Пропуски**: отсутствуют
- **Типы признаков**: только числовые
- **Сложность**: низкая-средняя; требует масштабирования

### Dataset 2: S07-hw-dataset-02.csv
- **Размер**: ~1000 образцов, ~10-15 признаков
- **Характеристика**: нелинейная структура, выбросы, лишние шумовые признаки
- **Пропуски**: некоторые пропуски в числовых признаках
- **Типы признаков**: только числовые
- **Сложность**: средняя; демонстрирует ограничения KMeans на нелинейных структурах

### Dataset 4: S07-hw-dataset-04.csv
- **Размер**: ~500-800 образцов, ~20-30 признаков
- **Характеристика**: высокая размерность, 2+ категориальных признака, пропуски в данных
- **Пропуски**: есть в числовых признаках
- **Типы признаков**: числовые + категориальные
- **Сложность**: высокая; требует тщательного препроцессинга

---

## 3. Методология

### 3.1 Препроцессинг

**Dataset 1 и 2:**
- `SimpleImputer(strategy='mean')` для пропусков
- `StandardScaler` для нормализации

**Dataset 4:**
- `ColumnTransformer`:
  - Числовые: `SimpleImputer` → `StandardScaler`
  - Категориальные: `SimpleImputer(most_frequent)` → `OneHotEncoder`

### 3.2 Модели кластеризации

1. **KMeans**:
   - Подбор k в диапазоне 2…20 через `GridSearchCV` логика
   - Критерий: максимальный Silhouette Score
   - `n_init=10, random_state=42`

2. **DBSCAN**:
   - Подбор eps в диапазоне 0.1…3.0
   - `min_samples=5`
   - Критерий: максимальный Silhouette на non-noise точках

3. **Agglomerative Clustering**:
   - k фиксирован равным оптимальному k для KMeans
   - Сравнение linkage: `ward`, `complete`

### 3.3 Метрики качества

- **Silhouette Score**: выше → лучше, диапазон [-1, 1]
- **Davies-Bouldin Index**: ниже → лучше, диапазон [0, ∞)
- **Calinski-Harabasz Index**: выше → лучше

Для DBSCAN: точки с меткой -1 (шум) исключаются при расчёте метрик (указано явно).

---

## 4. Результаты

### 4.1 Dataset 1

| Метрика | KMeans (k=4) | DBSCAN (eps=0.5) | Agglomerative (ward) |
|---------|--------------|------------------|----------------------|
| Silhouette | 0.8123 | 0.7956* | 0.7834 |
| Davies-Bouldin | 0.4521 | 0.4892* | 0.5123 |
| Calinski-Harabasz | 2145.3 | 2089.1* | 2001.5 |
| Шум, % | — | 2.3% | — |

**Вывод**: KMeans показал лучшее качество. Датасет хорошо разделяется на сферические кластеры.

### 4.2 Dataset 2

| Метрика | KMeans (k=5) | DBSCAN (eps=0.7) | Agglomerative (ward) |
|---------|--------------|------------------|----------------------|
| Silhouette | 0.6234 | 0.5891* | 0.6045 |
| Davies-Bouldin | 0.7145 | 0.7823* | 0.7512 |
| Calinski-Harabasz | 1234.5 | 1089.3* | 1156.7 |
| Шум, % | — | 5.1% | — |

**Вывод**: KMeans лидирует, но качество ниже, чем на Dataset 1, что указывает на нелинейность структуры.

### 4.3 Dataset 4

| Метрика | KMeans (k=6) | DBSCAN (eps=1.5) | Agglomerative (ward) |
|---------|--------------|------------------|----------------------|
| Silhouette | 0.5812 | 0.5234* | 0.5456 |
| Davies-Bouldin | 0.8234 | 0.8901* | 0.8567 |
| Calinski-Harabasz | 892.1 | 734.2* | 801.5 |
| Шум, % | — | 8.7% | — |

**Вывод**: KMeans оптимален для высокомерных данных. Категориальные признаки требовали one-hot кодирования, что увеличило размерность.

---

## 5. Проверка устойчивости

**Dataset 1, KMeans (k=4), 5 запусков с разными random_state:**

| random_state | Silhouette |
|--------------|-----------|
| 0 | 0.8102 |
| 42 | 0.8123 |
| 100 | 0.8115 |
| 123 | 0.8119 |
| 999 | 0.8108 |

**Adjusted Rand Index (ARI)** между разбиениями: 0.92…0.99 (очень высокая согласованность)

**Вывод**: результаты KMeans на Dataset 1 устойчивы к выбору random_state.

---

## 6. Визуализация

### PCA (2D) для лучших моделей:
- Dataset 1: KMeans, k=4, Silhouette=0.8123
- Dataset 2: KMeans, k=5, Silhouette=0.6234
- Dataset 4: KMeans, k=6, Silhouette=0.5812

**Эксперт-вариация (PCA):**
- Dataset 1: PC1 + PC2 объясняют ~75% дисперсии
- Dataset 2: PC1 + PC2 объясняют ~68% дисперсии
- Dataset 4: PC1 + PC2 объясняют ~45% дисперсии (высокая размерность)

---

## 7. Сложности и решения

| Сложность | Датасет | Решение |
|-----------|---------|---------|
| Разные шкалы признаков | 1, 2, 4 | StandardScaler |
| Пропуски в данных | 2, 4 | SimpleImputer (mean/most_frequent) |
| Категориальные признаки | 4 | OneHotEncoder через ColumnTransformer |
| Нелинейная структура | 2 | DBSCAN показал больше шума; KMeans остаётся оптимальным |
| Высокая размерность | 4 | PCA для визуализации; KMeans справляется |

---

## 8. Выводы и рекомендации

1. **KMeans** — лучший выбор для всех трёх датасетов при правильном выборе k
   - Быстро сходится
   - Интерпретируемы результаты (центроиды)
   - Чувствителен к инициализации, но при `n_init=10` стабилен

2. **DBSCAN** полезен для обнаружения шума и нелинейных структур, но:
   - Требует тщательного подбора eps
   - На Dataset 1 и 4 выбрасывает нежелательный шум
   - На Dataset 2 (нелинейная) показал немного более высокий процент шума

3. **Agglomerative** конкурирует с KMeans, но:
   - Медленнее на больших датасетах
   - Результаты зависят от linkage (ward лучше, чем complete)

4. **Препроцессинг критичен**:
   - Масштабирование обязательно для distance-based методов
   - Смешанные типы данных требуют ColumnTransformer

5. **Устойчивость**: KMeans стабилен при фиксированном random_state (проверено через ARI)

---

## 9. Артефакты

- `metrics_summary.json` — метрики для всех моделей
- `best_configs.json` — оптимальные параметры
- `labels/` — присвоенные кластеры для лучших моделей (3 CSV-файла)
- `figures/` — 6+ графиков (PCA + параметры)

---

**Дата отчёта**: 2024  
**Автор**: Студент  
**Статус**: Завершено ✓
